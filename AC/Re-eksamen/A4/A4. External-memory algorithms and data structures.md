modern hard drive specs:
* seek time: 4-10ms
* spindle speed: ~10K rpm => Half of ration: ~3ms
* transfer rate: 500 MB/s => Transferring 1 byte: 0.000003ms

takeaways from using hard disks:
* It makes sense to *read and write in large blocks* - disk pages (4-32Kb)
* *Sequential* access is much faster than random access
* Disk access is much slower than main-memory access

* It is generally a bad idea to use main-memory algorithm in external memory 
	* take a balanced binary search tree
	* if each node gets a separate disk page, which is a waist of space and search is just O(log2 N)
	* even if we fill the disk pages the search time may still be O(log2 N)

# B-trees
* In B-trees are we only concerned with keys
* the nodes have high fan-out(many children) = $\theta$(B)
	* *Degree* of a tree t:
		* Min_fan-out = t, Max_fan-out = 2\*t = B / index_entry_size
	* Root is the exception: can have as little as two children 
* B-tree is a balanced tree, and all leaves have *the same depth*: $h=\theta(\log_tN) = \theta(\log_BN)$

* internal nodes
	* t-1 to 2t-1 keys
	* pointer1 key1... pointerx keyx pointerx+1
	* key1 $\leq$ key2 ... $\leq$ keyx
	* pointer1.key $\leq$ key1 and keyx < pointerx+1.key
		* $key_{i-1} < pointer_i.key\leq key_i$

### Searching 
* The root node is normally "always" in main memory
	* no need to perform a DiskRead on the root
* Search is very similar to a search in a binary search tree
	* instead of making a binary branching decision at each node, we make a *(j+1)*-way branching decision, where j is just the number of keys in a node.

* pseudo code:
	* x is a node and x.n is the number of keys in the node
	* k is the key that we are searching for
	* x.key$_i$ is the i-th key of node x; and x.c$_i$ is the i-th pointer of node x![[Pasted image 20230603105017.png]]
	* Disk access: $O(h)=O(\log_tN)$
	* CPU: $O(th) = O(t\log_tN)$

## B$^+$-trees 
* is a variant of B-trees
	* all data keys are in leaf nodes
	* leaf-nodes are connected into a doubly linked list![[Pasted image 20230603110219.png]]
### Search 
* search is very similar to search in a binary search tree
	* always goes to a leaf
	* range searches are convenient 
	* cost: $\theta(\log_BN+k/B)$
### Insertion
* the algorithm for insertion is split up into different phases
	* Down-phase: recursively traverse down and find the leaf (as in search)
	* Up-phase: insert the key. If necessary, *split* nodes and propagate the splits up the tree
	
* assumption:
	* In the *down-phase* pointers to traversed nodes are saved in the stack as there are no parent pointers

### Node splitting 
* leaf node: copy the middel key to the parent![[Pasted image 20230603111030.png]]
* Internal node: move the middel key to the parent, as in B-tree![[Pasted image 20230603111126.png]]
* The tree grows when the root is split into two nodes and their parent becomes the new root

### Example of insert in B$^+$-tree
* Insert P (maximum fan-out = 5 children = 4 data entries)
* Down-phase: Leaf node![[Pasted image 20230603111627.png]]
* need to split, since fan-out is 5, which means at most 4 keys in a node![[Pasted image 20230603111750.png]]
* cost: $\theta(\log_BN)$

#### more examples 
![[Pasted image 20230603113527.png]]
![[Pasted image 20230603113536.png]]

### Deletion 
* Opposite of insertion:
	* phase 1: traverse down to find the key in a leaf
	* phase 2: remove the key and traverse up handling underfull nodes
		* underfull handling case: Distributed ![[Pasted image 20230603113905.png]]
		* case: merging ![[Pasted image 20230603113931.png]]

	* *Tree shrinking*: if the root has only child, then remove the root.


* R-tree and grow-post trees can be found on slide 25-26 in lec 4
	* R-tree: Generalisation of B$^+$-trees to multiple dimensions 
	* Grow-post trees: Generalised Search trees - GiST


# External-Memory
* *External-memory algorithm*: Is for when data do not fit in main memory
* *External-memory sorting*: The rough idea is to sort the data that fits in main-memory  and "merge"
## Merge sort
* the idea of External-memory merge sort is to increase the size of initial runs
	* Initial runs - the size of available main memory (**M** data elements)
![[Pasted image 20230806212847.png]]

* info
	* The total number of input elements that we want to sort is **N**
	* Available memory can hold **M** elements for in-memory merge sort.
	* A disk page can hold **B** elements
	* **N**>**M**>**B**
	* **n**=**N**/**B**: the total number of disk pages of the input
	* **m**=**M**/**B**: the total number of disk pages that fit in the available memory.

### Running Merge sort
* Input file **X**, empty file **Y**

* *Phase 1*: repeat until the end of file **X**: 
	* read the next **M** elements from **X**
	* sort them in main memory 
	* write them at the end of file **Y**

	* analysis: Read file **X**, write file **Y**: performs 2**n** I/Os ($\theta$**n** time)

* *phase 2*: repeat while there is more than one run in **Y**:
	* empty **X**
	* *mergeAllRuns*(**Y**,**X**)
	* **X** is now called **Y**, **Y** is now called **X**

	* analysis: 
		* One iteration: Read file **X**, write file **Y**: performs 2**n** I/Os ($\theta$**n** time)
		* Number of Iterations: Log$_2$**N**/**M** =Log$_2$**n**/**m**

* *mergeAllRuns*(**Y**,**X**): repeat until the end of **Y**
	* call *twowayMerge* to merge the next to runs from **Y** into one run, which is written at the end of **X**. 
	* *twowayMerge*: uses three main-memory arrays of size **B**:![[Pasted image 20230603120143.png]]

* Visulisation of The Phases:![[Pasted image 20230603120418.png]]

total run time: $\theta(n\log_2n/m)$

### Two Phase Multiway merge sort
* idea: merge all runs at once![[Pasted image 20230603120926.png]]

	* *Phase 1*: the same (do internal sorts)
	* *Phase 2*: perform MultiwayMerge(y,x)
		* multiway merging![[Pasted image 20230603121005.png]]

* analysis: 
	* *Phase 1*: $\theta$(**n**)
	* *Phase 2*: $\theta$(**n**)
	* which makes total run time $\theta$(n)
	
	* catch: can only be done files with a limited size
		* phase 2 can merge a maximum of m-1 runs
		* Which means: N/M $\leq$ m -1 (n/m $\leq$ m-1)


### General Multiway Merge Sort
* What if a file is **very** large or memory is small?

General multiway merge sort works by:
* *Phase 1*: Is the same (do internal sort)
* *Phase 2*: Do as many iterations of merging as necessary until only one run remains
	* Each iteration repeatedly calls *MultiwayMerge*(**Y**,**X**) to merge groups of **m**-1 runs until the end of file **Y** is reached![[Pasted image 20230808102807.png]]

analysis:
* phase 1: $\theta$(n)
* each iteration in phase 2:$\theta$(n)
	* the number of iterations: log$_{m-1}$**N**/**M** = $\theta$(log$_m$**n**)
* Total running time: $\theta$(n log$_m$ n) I/Os